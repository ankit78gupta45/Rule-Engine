Q.1. Briefly describe the conceptual approach you chose! What are the trade-offs?

Ans - The approach that I followed is in the following steps : 
	1. User will provide inputs in the following order and format 
              **signal : {must be Alphanumeric user_input e.g : ATL1}
              **conditionRule : one of these {>,<,<>,=}
	      **ValueType : {Either Integer or double, String, DateTime format: YYYY-MM-DD HH:MM:SS }
	      **Value : {e.g of Integer/Double - 23/23.3, e.g of String -Either "High" or "Low", e.g of DateTime : 22-12-12 12:34:12}
	2. After Taking the inputs from the user all the data will be stored in JSONObject through which it get written into a user-defined file.
	3. Now, the dataset in user-Defined file and Test(raw_data) file are matched according to different conditionRules { <, > , <>, =} which users have provided.
	4. If condition fails, then rule is violated hence, we say signal violated.
	4. If any signals are getting violated, then we put those signal in a HashSET.
	5. And, finally those violated signals get printed in the output screen.
	
	Trade-offs : 
	
	* JSON : JSONArray is used to insert dataset into the file and retrieve the dataset from the file and that dataset is stored as JSONObject.
	       JSON increases the search as it uses [key]:[value] pairs. 

	* Data Validation : All the data that user inputs is validated first(whether its an alphanumeric or not,
			  String accepts "High" or "Low" only and DateTime shouldn't be wrong valued),
 		          then only those dataset are inserted into the JSONobject.
	* Exception Handling: In case of exception, the program terminated and tells why the exception occurred. 
			    Mostly beacause of the wrong path provided, exception came in my case. 
 
Q.2. What's the runtime performance? What is the complexity? Where are the bottlenecks?

Ans -   Runtime Performance of this ruleEngine is much faster as I have implemented it in Java which uses JIT compiler .
	The time taken by this rule Engine to perform the task is around ~ 20 ms for inputs less than 10, after user have input all the values.
	Also, as every dataset in the UserRule Inputs is matched with every inputs in raw_data inputs,
	it's complexity should be (No. of raw_data inputs * No. of UserRule inputs).
	
	Bottleneck : 
	1. It could be the time taken to input the rules by the users is high.
	2. To input more than 100 rules its going to takes a lot of time by the user.

Q.3. If you had more time, what improvements would you make, and in what order of priority?

Ans -    ** I have Implemented first as a normal rule engine which can take any values,
	 then i have improved it and added data validation to it,
	 it wont accept if any wrong data tries to insert by user.

	 ** If I had more time I would have applied Data Mining and Learning to the engine,
	 so that the user rules can be interpreted by the machine and we need not enter the fields specifically. 
	
